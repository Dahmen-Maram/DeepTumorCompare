{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LZbU_3h3iIzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f270a6-f9ca-40ab-844d-f203db16953c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/8.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m7.5/8.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/813.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.8/165.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Colab Cell 2 — Installer paquets\n",
        "!pip install -q streamlit==1.22.0 pyngrok tensorflow Pillow numpy\n",
        "# (la version de tensorflow peut être ajustée si nécessaire)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 5 — Lancer Streamlit en arrière-plan et créer tunnel ngrok\n",
        "import os, time, subprocess, sys\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- CONFIG ---\n",
        "NGROK_AUTHTOKEN = \"2vdJFZExAVhUhUeOJZjhTHOLFhG_3noX2p3A3rDQ3KYjAKFgF\"  # remplace si nécessaire\n",
        "PORT = 8501\n",
        "LOGFILE = \"/content/streamlit.log\"\n",
        "\n",
        "# 1) Auth token ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# 2) Kill d'anciennes instances (sécuritaire)\n",
        "try:\n",
        "    !pkill -f streamlit\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "time.sleep(1)\n",
        "\n",
        "# 3) Lancer streamlit dans un subprocess (headless)\n",
        "cmd = [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port={}\".format(PORT), \"--server.headless=true\"]\n",
        "print(\"Lancement de Streamlit :\", \" \".join(cmd))\n",
        "f = open(LOGFILE, \"wb\")\n",
        "proc = subprocess.Popen(cmd, stdout=f, stderr=subprocess.STDOUT)\n",
        "\n",
        "# 4) Attendre un peu que streamlit démarre\n",
        "print(\"PID Streamlit:\", proc.pid)\n",
        "print(\"Attente de démarrage (10s)...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# 5) Créer tunnel ngrok\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "print(\"Streamlit public URL:\", public_url)\n",
        "print(\"Logs: \", LOGFILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raS-6nZSk8pW",
        "outputId": "d2757eaa-37a1-4b29-c338-fe83b3868841"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancement de Streamlit : /usr/bin/python3 -m streamlit run app.py --server.port=8501 --server.headless=true\n",
            "PID Streamlit: 949\n",
            "Attente de démarrage (10s)...\n",
            "Streamlit public URL: https://c2c0c894df4c.ngrok-free.app\n",
            "Logs:  /content/streamlit.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f ngrok && fuser -k 8501/tcp && echo \"✅ Nettoyage terminé\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsWPHlRvlmTH",
        "outputId": "dced9998-0f64-431a-9952-05e332ae481d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-10-21T13:31:05+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-d0ebdb8c-9c05-4917-be74-6db7965c88ed acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import io\n",
        "import traceback\n",
        "#VERSIONN FINAAALEEE\n",
        "# ----- CONFIG PATHS -----\n",
        "DENSENET_PATH = \"/content/drive/MyDrive/TumourDL/densenet_brain_tumor.h5\"\n",
        "VGG16_PATH = \"/content/drive/MyDrive/TumourDL/vgg16_brain_tumor.h5\"\n",
        "RESNET_PATH = \"/content/drive/MyDrive/Copie de resnet152v2_brain_tumor.h5\"\n",
        "# Ton fichier Xception — peut être .keras (full model) ou un fichier de weights compatible\n",
        "XCEPTION_PATH = \"/content/drive/MyDrive/TumourDL/xception_brain_tumor.keras\"\n",
        "\n",
        "# ----- SETTINGS -----\n",
        "IMG_SIZE = (224, 224)\n",
        "XCEPTION_SIZE = (299, 299)\n",
        "CLASS_NAMES = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "st.set_page_config(page_title=\"🧠 Brain Tumor Classifier\", layout=\"wide\", page_icon=\"🧠\")\n",
        "st.title(\"🧠 Brain Tumor Classifier — DenseNet121, VGG16, ResNet152V2 & Xception\")\n",
        "st.markdown(\"Upload an image, choose un modèle, et voir les prédictions et comparatifs.\")\n",
        "\n",
        "# ----- Robust Xception loader -----\n",
        "def try_load_xception(path, xception_size=XCEPTION_SIZE, num_classes=len(CLASS_NAMES)):\n",
        "    \"\"\"\n",
        "    1) Try load_model(path) — works if path is a full saved model (.keras or full .h5 model)\n",
        "    2) If that fails, rebuild Xception with input_shape=(299,299,3) and try model.load_weights(path)\n",
        "    \"\"\"\n",
        "    tb_full = tb_weights = \"\"\n",
        "    # 1) try load_model (full model)\n",
        "    try:\n",
        "        m = load_model(path)\n",
        "        return m\n",
        "    except Exception as e_full:\n",
        "        tb_full = traceback.format_exc()\n",
        "        # continue to try weights approach\n",
        "\n",
        "    # 2) try rebuild architecture and load weights\n",
        "    try:\n",
        "        base = Xception(weights=None, include_top=False, input_shape=(xception_size[0], xception_size[1], 3))\n",
        "        x = layers.GlobalAveragePooling2D()(base.output)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "        model = Model(inputs=base.input, outputs=outputs)\n",
        "        model.load_weights(path)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "    except Exception as e_weights:\n",
        "        tb_weights = traceback.format_exc()\n",
        "\n",
        "    # both failed => raise informative RuntimeError\n",
        "    raise RuntimeError(\n",
        "        \"Both load_model() and rebuild+load_weights() failed for Xception.\\n\\n\"\n",
        "        f\"load_model() traceback:\\n{tb_full}\\n\\nrebuild+load_weights() traceback:\\n{tb_weights}\\n\\n\"\n",
        "        \"Causes possibles:\\n\"\n",
        "        \"- Le fichier n'est ni un modèle complet ni un fichier de poids compatibles.\\n\"\n",
        "        \"- Le fichier de poids a été sauvegardé depuis une architecture différente.\\n\\n\"\n",
        "        \"Solutions recommandées:\\n\"\n",
        "        \"- Si tu as le script d'entraînement : `model.save('/path/xception_brain_tumor.keras')` (modèle complet).\\n\"\n",
        "        \"- Ou sauvegarde les poids avec `model.save_weights('/path/xception_weights.h5')` et fournis ces poids.\\n\"\n",
        "    )\n",
        "\n",
        "# ----- Cached loader -----\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_model_cached(path, model_name=None):\n",
        "    if model_name == \"Xception\":\n",
        "        return try_load_xception(path)\n",
        "    return load_model(path)\n",
        "\n",
        "# ----- Load models -----\n",
        "dense_model = vgg_model = resnet_model = xception_model = None\n",
        "missing = []\n",
        "\n",
        "try:\n",
        "    dense_model = load_model_cached(DENSENET_PATH)\n",
        "except Exception as e:\n",
        "    missing.append((\"DenseNet121\", DENSENET_PATH, str(e)))\n",
        "\n",
        "try:\n",
        "    vgg_model = load_model_cached(VGG16_PATH)\n",
        "except Exception as e:\n",
        "    missing.append((\"VGG16\", VGG16_PATH, str(e)))\n",
        "\n",
        "try:\n",
        "    resnet_model = load_model_cached(RESNET_PATH)\n",
        "except Exception as e:\n",
        "    missing.append((\"ResNet152V2\", RESNET_PATH, str(e)))\n",
        "\n",
        "try:\n",
        "    xception_model = load_model_cached(XCEPTION_PATH, model_name=\"Xception\")\n",
        "except Exception as e:\n",
        "    missing.append((\"Xception\", XCEPTION_PATH, str(e)))\n",
        "\n",
        "if missing:\n",
        "    st.warning(\"Certains modèles n'ont pas pu être chargés — vérifie les chemins et formats. Détails :\")\n",
        "    for name, path, err in missing:\n",
        "        st.markdown(f\"**{name}** — `{path}`\")\n",
        "        # show first lines of error to keep UI compact\n",
        "        first_lines = \"\\n\".join(str(err).splitlines()[:12])\n",
        "        st.code(first_lines, language=\"text\")\n",
        "        st.markdown(\"---\")\n",
        "    st.info(\"Si possible, fournis un modèle complet `.keras` ou un fichier de poids compatible et la même architecture.\")\n",
        "\n",
        "# ----- Preprocessing (adaptée au modèle choisi) -----\n",
        "def preprocess_image_for_model(img: Image.Image, model_name=None):\n",
        "    \"\"\"\n",
        "    Convertit en RGB, ajuste la taille en fonction du modèle (Xception=299, autres=224), normalise et ajoute batch dim.\n",
        "    \"\"\"\n",
        "    if img.mode != \"RGB\":\n",
        "        img = img.convert(\"RGB\")\n",
        "\n",
        "    if model_name == \"Xception\":\n",
        "        target_size = XCEPTION_SIZE\n",
        "    else:\n",
        "        target_size = IMG_SIZE\n",
        "\n",
        "    img = ImageOps.fit(img, target_size, method=Image.Resampling.LANCZOS)\n",
        "    arr = np.array(img).astype(\"float32\") / 255.0\n",
        "    arr = np.expand_dims(arr, axis=0)\n",
        "    return arr\n",
        "\n",
        "# ----- Prediction (utilise model_choice) -----\n",
        "def predict_model(model, img, model_name=None):\n",
        "    if model is None:\n",
        "        raise ValueError(\"Model is None — impossible de prédire.\")\n",
        "    x = preprocess_image_for_model(img, model_name=model_name)\n",
        "    preds = model.predict(x)\n",
        "    if preds.ndim == 2:\n",
        "        preds = preds[0]\n",
        "    top_idx = preds.argsort()[::-1]\n",
        "    return preds, top_idx\n",
        "\n",
        "# ----- Sidebar -----\n",
        "st.sidebar.header(\"⚙️ Options\")\n",
        "show_topk = st.sidebar.slider(\"Afficher top-k probabilités\", 1, len(CLASS_NAMES), 3)\n",
        "threshold = st.sidebar.slider(\"Seuil d'affichage (probabilité minimale)\", 0.0, 1.0, 0.0, 0.01)\n",
        "\n",
        "# ----- Upload + Prediction -----\n",
        "st.header(\"1️⃣ Upload & Prédiction\")\n",
        "uploaded_file = st.file_uploader(\"Choisis une image (jpg/png)\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(uploaded_file.read()))\n",
        "    except Exception as e:\n",
        "        st.error(f\"Erreur lors de la lecture de l'image : {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    col1, col2 = st.columns([1,1])\n",
        "    with col1:\n",
        "        st.image(image, caption=\"Image uploadée\", use_column_width=True)\n",
        "\n",
        "    with col2:\n",
        "        available_models = []\n",
        "        if dense_model is not None: available_models.append(\"DenseNet121\")\n",
        "        if vgg_model is not None: available_models.append(\"VGG16\")\n",
        "        if resnet_model is not None: available_models.append(\"ResNet152V2\")\n",
        "        if xception_model is not None: available_models.append(\"Xception\")\n",
        "\n",
        "        if not available_models:\n",
        "            st.error(\"Aucun modèle disponible — corrige les chemins ou formats et relance.\")\n",
        "        else:\n",
        "            model_choice = st.radio(\"Choisir le modèle pour la prédiction\", available_models)\n",
        "            model = {\n",
        "                \"DenseNet121\": dense_model,\n",
        "                \"VGG16\": vgg_model,\n",
        "                \"ResNet152V2\": resnet_model,\n",
        "                \"Xception\": xception_model\n",
        "            }[model_choice]\n",
        "\n",
        "            preds, top_idx = predict_model(model, image, model_name=model_choice)\n",
        "\n",
        "            st.markdown(\"### Top probabilités :\")\n",
        "            for i in range(show_topk):\n",
        "                idx = top_idx[i]\n",
        "                prob = float(preds[idx])\n",
        "                label = CLASS_NAMES[idx]\n",
        "                if prob >= threshold:\n",
        "                    st.success(f\"{i+1}. {label} — {prob*100:.2f}%\")\n",
        "                else:\n",
        "                    st.warning(f\"{i+1}. {label} — {prob*100:.2f}% (sous seuil)\")\n",
        "\n",
        "            best_i = top_idx[0]\n",
        "            best_prob = float(preds[best_i])\n",
        "            best_label = CLASS_NAMES[best_i]\n",
        "            if best_prob >= threshold:\n",
        "                st.success(f\"✅ Prédiction principale: {best_label} ({best_prob*100:.2f}%)\")\n",
        "            else:\n",
        "                st.warning(f\"⚠️ Prédiction principale: {best_label} ({best_prob*100:.2f}%) sous le seuil\")\n",
        "\n",
        "# ----- Comparatif -----\n",
        "st.header(\"2️⃣ Comparatif des modèles\")\n",
        "comparative_data = {\n",
        "    \"Modèle\": [\"DenseNet121\", \"VGG16\", \"ResNet152V2\", \"Xception\"],\n",
        "    \"Accuracy Test (%)\": [83.83, 88.25, 93.97, 70.00],\n",
        "    \"Précision (%)\": [84.0, 88.5, 94.40, 68.70],\n",
        "    \"Recall (%)\": [83.0, 87.5, 93.97, 70.00]\n",
        "}\n",
        "df_compare = pd.DataFrame(comparative_data)\n",
        "st.table(df_compare)\n",
        "\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Made with Streamlit 🧠\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJV9WS1pPaRI",
        "outputId": "73aa3587-bf26-4168-cd09-c8e0a162370e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}